{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/jinsuyoo/TalkingHead-1KH/val/cropped_clips/1lSejjfNHpw_0075_S0_E728_L671_T47_R1471_B847.mp4\n",
      "width: 800 height: 800 num_frames: 729 fps: 25.0\n",
      "1lSejjfNHpw_0075_S0_E728_L671_T47_R1471_B847\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jinsuyoo/vglean-private/scripts/video_processing/talkinghead1kh_vid2frame.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.104.110.183/home/jinsuyoo/vglean-private/scripts/video_processing/talkinghead1kh_vid2frame.ipynb#ch0000000vscode-remote?line=34'>35</a>\u001b[0m hq_raw \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mflip(frame, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.104.110.183/home/jinsuyoo/vglean-private/scripts/video_processing/talkinghead1kh_vid2frame.ipynb#ch0000000vscode-remote?line=35'>36</a>\u001b[0m hq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(resize(hq_raw, out_shape\u001b[39m=\u001b[39m(\u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m)), \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.104.110.183/home/jinsuyoo/vglean-private/scripts/video_processing/talkinghead1kh_vid2frame.ipynb#ch0000000vscode-remote?line=36'>37</a>\u001b[0m lq \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(resize(hq, out_shape\u001b[39m=\u001b[39;49m(\u001b[39m32\u001b[39;49m, \u001b[39m32\u001b[39;49m)), \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.104.110.183/home/jinsuyoo/vglean-private/scripts/video_processing/talkinghead1kh_vid2frame.ipynb#ch0000000vscode-remote?line=38'>39</a>\u001b[0m hq \u001b[39m=\u001b[39m (hq \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mround()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.104.110.183/home/jinsuyoo/vglean-private/scripts/video_processing/talkinghead1kh_vid2frame.ipynb#ch0000000vscode-remote?line=39'>40</a>\u001b[0m hq \u001b[39m=\u001b[39m hq[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/ResizeRight/resize_right.py:118\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(input, scale_factors, out_shape, interp_method, support_sz, antialiasing, by_convs, scale_tolerance, max_numerator, pad_mode)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=110'>111</a>\u001b[0m \u001b[39m# STEP 4- APPLY WEIGHTS: Each output pixel is calculated by multiplying\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=111'>112</a>\u001b[0m \u001b[39m# its set of weights with the pixel values in its field of view.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=112'>113</a>\u001b[0m \u001b[39m# We now multiply the fields of view with their matching weights.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=113'>114</a>\u001b[0m \u001b[39m# We do this by tensor multiplication and broadcasting.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=114'>115</a>\u001b[0m \u001b[39m# if by_convs is true for this dim, then we do this action by\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=115'>116</a>\u001b[0m \u001b[39m# convolutions. this is equivalent but faster.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=116'>117</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dim_by_convs:\n\u001b[0;32m--> <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=117'>118</a>\u001b[0m     output \u001b[39m=\u001b[39m apply_weights(output, field_of_view, weights, dim, n_dims,\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=118'>119</a>\u001b[0m                            pad_sz, pad_mode, fw)\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=119'>120</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=120'>121</a>\u001b[0m     output \u001b[39m=\u001b[39m apply_convs(output, scale_factor, in_sz, out_sz, weights,\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=121'>122</a>\u001b[0m                          dim, pad_sz, pad_mode, fw)\n",
      "File \u001b[0;32m~/ResizeRight/resize_right.py:235\u001b[0m, in \u001b[0;36mapply_weights\u001b[0;34m(input, field_of_view, weights, dim, n_dims, pad_sz, pad_mode, fw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=223'>224</a>\u001b[0m tmp_input \u001b[39m=\u001b[39m fw_pad(tmp_input, fw, pad_sz, pad_mode)\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=225'>226</a>\u001b[0m \u001b[39m# field_of_view is a tensor of order 2: for each output (1d location\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=226'>227</a>\u001b[0m \u001b[39m# along cur dim)- a list of 1d neighbors locations.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=227'>228</a>\u001b[0m \u001b[39m# note that this whole operations is applied to each dim separately,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=232'>233</a>\u001b[0m \u001b[39m# consider neighbors along the current dim, but such set exists for every\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=233'>234</a>\u001b[0m \u001b[39m# multi-dim location, hence the final tensor order is image_dims+1.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=234'>235</a>\u001b[0m neighbors \u001b[39m=\u001b[39m tmp_input[field_of_view]\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=236'>237</a>\u001b[0m \u001b[39m# weights is an order 2 tensor: for each output location along 1d- a list\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=237'>238</a>\u001b[0m \u001b[39m# of weights matching the field of view. we augment it with ones, for\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=238'>239</a>\u001b[0m \u001b[39m# broadcasting, so that when multiplies some tensor the weights affect\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=239'>240</a>\u001b[0m \u001b[39m# only its first dim.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jinsuyoo/ResizeRight/resize_right.py?line=240'>241</a>\u001b[0m tmp_weights \u001b[39m=\u001b[39m fw\u001b[39m.\u001b[39mreshape(weights, (\u001b[39m*\u001b[39mweights\u001b[39m.\u001b[39mshape, \u001b[39m*\u001b[39m [\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m (n_dims \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import mmcv\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jinsuyoo/ResizeRight')\n",
    "from resize_right import resize\n",
    "\n",
    "\n",
    "save_path = '../../datasets/TalkingHead-1KH_val/'\n",
    "#os.makedirs(osp.join(save_path, 'GT'))\n",
    "#os.makedirs(osp.join(save_path, 'LQ'))\n",
    "\n",
    "video_paths = sorted(\n",
    "    glob.glob(osp.join('/data/jinsuyoo/TalkingHead-1KH/val/cropped_clips/*.mp4')))\n",
    "\n",
    "for video_path in video_paths:\n",
    "    print(video_path)\n",
    "    video = mmcv.VideoReader(video_path)\n",
    "    print('width:', video.width, 'height:', video.height, 'num_frames:', len(video), 'fps:', video.fps)\n",
    "    \n",
    "    filename, _ = osp.splitext(osp.basename(video_path))\n",
    "    print(filename)\n",
    "\n",
    "    os.makedirs(osp.join(save_path, 'GT', filename), exist_ok=True)\n",
    "    os.makedirs(osp.join(save_path, 'LQ', filename), exist_ok=True)\n",
    "\n",
    "    for i, frame in enumerate(video):\n",
    "        hq_raw = np.flip(frame, axis=2) / 255\n",
    "        hq = np.clip(resize(hq_raw, out_shape=(512, 512)), 0, 1)\n",
    "\n",
    "        hq = (hq * 255).round().astype(np.uint8)\n",
    "        hq = hq[..., ::-1]\n",
    "        cv2.imwrite(osp.join(save_path, 'GT', filename, f'{i:05d}.png'), hq)\n",
    "\n",
    "        lq = hq[..., ::-1] / 255\n",
    "        lq = np.clip(resize(lq, out_shape=(32, 32)), 0, 1)\n",
    "\n",
    "        lq = (lq * 255).round().astype(np.uint8)\n",
    "        lq = lq[..., ::-1]\n",
    "    \n",
    "        cv2.imwrite(osp.join(save_path, 'LQ', filename, f'{i:05d}.png'), lq)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56a3729746d31e22e587350a9406b9a3d46fc2cbf9e52cf6a47757d697db936a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('vsr-lightning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
