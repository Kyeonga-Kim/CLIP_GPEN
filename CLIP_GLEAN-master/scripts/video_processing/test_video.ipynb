{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from http path: http://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-ffhq-config-f-official_20210327_171224-bce9310c.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 16:45:22,571 - mmedit - INFO - Load pretrained model from http://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-ffhq-config-f-official_20210327_171224-bce9310c.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from http path: http://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-ffhq-config-f-official_20210327_171224-bce9310c.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 16:45:24,684 - mmedit - INFO - Load pretrained model from http://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-ffhq-config-f-official_20210327_171224-bce9310c.pth\n",
      "2022-03-23 16:45:26,580 - mmedit - INFO - load checkpoint from torchvision path: torchvision://vgg16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: ../pretrained_model/glean_ffhq_16x_20210527-61a3afad.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 16:45:34,060 - mmgen - INFO - Switch to evaluation style mode: single\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jinsuyoo/mmediting/jinsu/test_video.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136362e3130342e3131302e3138332841363030305f3829227d/home/jinsuyoo/mmediting/jinsu/test_video.ipynb#ch0000000vscode-remote?line=70'>71</a>\u001b[0m img_lq \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlq\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136362e3130342e3131302e3138332841363030305f3829227d/home/jinsuyoo/mmediting/jinsu/test_video.ipynb#ch0000000vscode-remote?line=71'>72</a>\u001b[0m \u001b[39mfor\u001b[39;00m lq \u001b[39min\u001b[39;00m img_lq:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136362e3130342e3131302e3138332841363030305f3829227d/home/jinsuyoo/mmediting/jinsu/test_video.ipynb#ch0000000vscode-remote?line=72'>73</a>\u001b[0m     lq \u001b[39m=\u001b[39m lq\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136362e3130342e3131302e3138332841363030305f3829227d/home/jinsuyoo/mmediting/jinsu/test_video.ipynb#ch0000000vscode-remote?line=73'>74</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223136362e3130342e3131302e3138332841363030305f3829227d/home/jinsuyoo/mmediting/jinsu/test_video.ipynb#ch0000000vscode-remote?line=74'>75</a>\u001b[0m         output \u001b[39m=\u001b[39m model(lq\u001b[39m=\u001b[39mlq, test_mode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import mmcv\n",
    "import torch\n",
    "import cv2\n",
    "from mmedit.apis import init_model, restoration_inference\n",
    "from mmedit.core import tensor2img\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jinsuyoo/ResizeRight')\n",
    "from resize_right import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "config = '../configs/restorers/glean/glean_ffhq_16x.py'\n",
    "checkpoint = '../pretrained_model/glean_ffhq_16x_20210527-61a3afad.pth'\n",
    "video_path = '/data/jinsuyoo/datasets/MEAD/M009/video/front/happy/level_1/001.mp4'\n",
    "save_path = 'result_face16_celeba'\n",
    "device = 0\n",
    "\n",
    "#img_paths = glob.glob(osp.join(dir_path, '*'))\n",
    "video_reader = mmcv.VideoReader(video_path)\n",
    "\n",
    "gt_size = 1024\n",
    "\n",
    "data = dict(hq_raw=[], hq_path=None, hq=[], lq=[])\n",
    "for frame in video_reader:\n",
    "    hq_raw = np.flip(frame, axis=2)\n",
    "    h, w, _ = hq_raw.shape\n",
    "    hq = hq_raw[(h-gt_size)//2:(h-gt_size)//2 + gt_size, (w-gt_size)//2:(w-gt_size)//2 + gt_size, :]\n",
    "    lq = resize(hq, scale_factors=1/16)\n",
    "    data['hq_raw'].append(hq_raw)\n",
    "    data['hq'].append(hq)\n",
    "    data['lq'].append(lq)\n",
    "    #print(hq_raw.shape, hq.shape, lq.shape)\n",
    "\n",
    "#sample_hq = data['hq'][0]\n",
    "\n",
    "#plt.figure()\n",
    "#plt.imshow(sample_hq)\n",
    "\n",
    "#gt_size = 1024\n",
    "#h, w, _ = sample_hq.shape\n",
    "#sample_hq_crop = sample_hq[(h-gt_size)//2:(h-gt_size)//2 + gt_size, (w-gt_size)//2:(w-gt_size)//2 + gt_size, :]\n",
    "\n",
    "#plt.figure()\n",
    "#plt.imshow(sample_hq_crop)\n",
    "\n",
    "#print(f'raw hq size: {sample_hq.shape}, cropped hq size: {sample_hq_crop.shape}')\n",
    "\n",
    "input('pause here')\n",
    "\n",
    "model = init_model(\n",
    "    config, checkpoint, device=torch.device('cuda', device))\n",
    "\n",
    "##################\n",
    "\n",
    "'''\n",
    "for i, img_path in enumerate(img_paths):\n",
    "    output = restoration_inference(model, img_path)\n",
    "    output = tensor2img(output)\n",
    "    \n",
    "    filename, extension = osp.splitext(osp.basename(img_path))\n",
    "    mmcv.imwrite(output, osp.join(save_path, f'{filename}_glean{extension}'))\n",
    "'''\n",
    "result = []\n",
    "img_lq = data['lq']\n",
    "for lq in img_lq:\n",
    "    lq = \n",
    "    lq = lq.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(lq=lq, test_mode=True)\n",
    "    output = output['output'].cpu()\n",
    "    result.append(output)\n",
    "    print(output.shape)\n",
    "result = torch.stack(result, dim=1)\n",
    "print(result.shape)\n",
    "\n",
    "h, w = result.shape[-2:]\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(save_path, fourcc, 25, (w, h))\n",
    "for i in range(0, result.size(1)):\n",
    "    img = tensor2img(result[:, i, :, :, :])\n",
    "    video_writer.write(img.astype(np.uint8))\n",
    "cv2.destroyAllWindows()\n",
    "video_writer.release()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13d3e73ef95eec664f398331fa6d4b5ac9634a4c67fad039117a3c9fbb295495"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mmedit': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
